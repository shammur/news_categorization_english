{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import warnings\n",
    "import datetime\n",
    "import optparse\n",
    "import os, errno\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance(object):\n",
    "    def __init__(self,id=1,img_path=\"\",damage_type_human=\"\",damage_type_classified=\"\",severity_lab_hum=\"\",severity_label_classified=\"\"):\n",
    "        self.id = id\n",
    "        self.image_path = img_path\n",
    "        self.damage_type_human = damage_type_human\n",
    "        self.damage_type_classified = damage_type_classified;\n",
    "        self.severity_label_human = severity_lab_hum\n",
    "        self.severity_label_classified = severity_label_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29581\n",
      "28495\n",
      "1086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "print(len(data_dict))\n",
    "\n",
    "\n",
    "out_file_name_dont_know = \"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data_dont_know_cant_judge.csv\"\n",
    "out_file_dont_know = open(out_file_name_dont_know,\"w\")\n",
    "out_file_dont_know.write(\"image,machine_label,human_label\\n\")\n",
    "\n",
    "out_file_name=\"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data_wo_dup.tsv\"\n",
    "out_file=open(out_file_name,\"w\")\n",
    "out_file.write(\"id\\timage_path\\tdamage_type_human\\tdamage_type_classified\\tdamage_level_classified\\tdamage_level_human\" + \"\\n\")\n",
    "damage_type_human_list = []\n",
    "damage_type_classified_list = []\n",
    "damage_level_human_list = []\n",
    "damage_level_classified_list = []\n",
    "\n",
    "dont_know_human=[]\n",
    "dont_know_machine=[]\n",
    "\n",
    "id_dup = []\n",
    "dup_entries={}\n",
    "count_inst=0\n",
    "count_inst=0\n",
    "for data in data_dict:\n",
    "\n",
    "    annotation_json=data['info']\n",
    "    image_url = annotation_json['file_id']\n",
    "\n",
    "    damage_type_human = annotation_json['damage_type_human'].strip() \n",
    "    damage_type_classified = annotation_json['damage_type_classified'].strip()\n",
    "    damage_level_human = annotation_json['damage_level_human'].strip()\n",
    "    damage_level_classified = annotation_json['damage_level_classified'].strip()\n",
    "\n",
    "    base_name = os.path.basename(image_url)\n",
    "    id = os.path.splitext(base_name)[0]\n",
    "    img_id = os.path.splitext(base_name)[0].strip()\n",
    "    image_path = image_url.replace(\"https://aidr-dev2.qcri.org/hurricane_dorian/\",\"/export/aidr-dev-data-02/aidr_data/persister/190830201918_hurricane_dorian_2019/\")\n",
    "\n",
    "    if(annotation_json['damage_type_human'].strip()==\"dont_know_or_cant_judge\" ):\n",
    "        #print(\"Ignoring this entry, \"+id+\" \"+image_url)\n",
    "        dont_know_human.append(\"dont_know_or_cant_judge\")\n",
    "        dont_know_machine.append(annotation_json['damage_type_classified'].strip())          \n",
    "        out_file_dont_know.write(image_url+\",\"+annotation_json['damage_level_classified'].strip()+\",\"+damage_type_human+\"\\n\")\n",
    "        continue\n",
    "        \n",
    "    if(damage_type_human!=\"\"):\n",
    "        damage_type_human = damage_type_human.replace(\"_damage\",\"\")\n",
    "        damage_type_classified = damage_type_classified.replace(\"_damage\",\"\")\n",
    "        damage_type_human_list.append(damage_type_human)\n",
    "        damage_type_classified_list.append(damage_type_classified)\n",
    "    else:\n",
    "        print(\"damage type has not annotated\")        \n",
    "\n",
    "    if(annotation_json['damage_type_human']==\"damage\"):\n",
    "        if(damage_level_classified!=\"\"):\n",
    "            damage_level_classified = damage_level_classified.replace(\"_damage\",\"\")\n",
    "            damage_level_classified_list.append(damage_level_classified)\n",
    "        else:\n",
    "            print(\"damage level (classified) has not annotated\")        \n",
    "\n",
    "        if(damage_level_human!=\"\"):\n",
    "            damage_level_human = damage_level_human.replace(\"_damage\",\"\")        \n",
    "            damage_level_human_list.append(damage_level_human)\n",
    "            \n",
    "    elif(annotation_json['damage_type_human']==\"no_damage\"):\n",
    "        damage_level_human =\"none\"\n",
    "        damage_level_human_list.append(damage_level_human)        \n",
    "        if(damage_level_classified!=\"\"):\n",
    "            damage_level_classified = damage_level_classified.replace(\"_damage\",\"\")\n",
    "            damage_level_classified_list.append(damage_level_classified)\n",
    "        else:\n",
    "            print(\"damage level (classified) has not annotated\")        \n",
    "\n",
    "#     if(id in id_dup):\n",
    "        #print(\"damage level has not annotated \"+damage_type_human)\n",
    "        #inst = Instance(id=img_id,img_path=image_path,severity_lab_hum=damage_level_human,severity_label_classified=damage_level_classified)\n",
    "        #out_file_dup.write(id+\"\\t\"+image_path+\"\\t\"+annotation_json['damage_type_human']+\"\\t\"+damage_type_classified+\"\\t\"+damage_level_human+\"\\t\"+damage_level_classified+\"\\n\")    \n",
    "#     if(img_id==\"1169511214520700928_0\"):\n",
    "#         print(id+\"\\t\"+image_path+\"\\t\"+annotation_json['damage_type_human']+\"\\t\"+damage_type_classified+\"\\t\"+damage_level_human+\"\\t\"+damage_level_classified+\"\\n\")\n",
    "    count_inst=count_inst+1   \n",
    "    if(img_id in dup_entries):\n",
    "        inst_list = dup_entries[img_id]\n",
    "        inst = Instance(id=img_id,img_path=image_path,damage_type_human=annotation_json['damage_type_human'],damage_type_classified=damage_type_classified,severity_lab_hum=damage_level_human,severity_label_classified=damage_level_classified)        \n",
    "        inst_list.append(inst)\n",
    "        dup_entries[img_id]=inst_list\n",
    "    else: \n",
    "        inst = Instance(id=img_id,img_path=image_path,damage_type_human=annotation_json['damage_type_human'],damage_type_classified=damage_type_classified,severity_lab_hum=damage_level_human,severity_label_classified=damage_level_classified)        \n",
    "        inst_list=[]\n",
    "        inst_list.append(inst)\n",
    "        dup_entries[img_id]=inst_list\n",
    "#     else:\n",
    "#         id_dup.append(id)\n",
    "        #out_file.write(id+\"\\t\"+image_path+\"\\t\"+annotation_json['damage_type_human']+\"\\t\"+damage_type_classified+\"\\t\"+damage_level_human+\"\\t\"+damage_level_classified+\"\\n\")\n",
    "        \n",
    "    \n",
    "out_file_dont_know.close()\n",
    "out_file.close()\n",
    "# out_file_dup.close()\n",
    "print(count_inst)\n",
    "print(len(dont_know_machine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save items to json file \n",
    "import json\n",
    "out_file_name_json = \"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data_json.json\"\n",
    "out_file=open(out_file_name_json,\"w\")\n",
    "for data in data_dict:\n",
    "    out_file.write(json.dumps(data)+\"\\n\");\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28495\n",
      "28454\n"
     ]
    }
   ],
   "source": [
    "value = 29581-1086\n",
    "print(value)\n",
    "value2 = value-41\n",
    "print(value2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28091\n",
      "41\n",
      "404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({u'mild-mild-': 9,\n",
       "         u'mild-none-': 4,\n",
       "         u'mild-severe-': 11,\n",
       "         u'none-mild-': 17,\n",
       "         'none-none-': 333,\n",
       "         u'none-severe-': 7,\n",
       "         u'severe-mild-': 1,\n",
       "         u'severe-none-': 1,\n",
       "         u'severe-severe-': 21})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(dup_entries))\n",
    "dup_entries_ve={} \n",
    "dis_agree_list={}\n",
    "\n",
    "agree_label_bucket=[]\n",
    "uniq_id={}\n",
    "for k,val in dup_entries.iteritems():\n",
    "    label_list = []\n",
    "    if(len(val)>=2):\n",
    "        label=\"\"\n",
    "        for inst in val:\n",
    "            uniq_id[inst.id]=inst.id\n",
    "            if(inst.severity_label_human not in label_list):\n",
    "                label_list.append(inst.severity_label_human)\n",
    "            label=inst.severity_label_human+\"-\"+label\n",
    "        agree_label_bucket.append(label)\n",
    "        if(len(label_list)>=2):\n",
    "            dis_agree_list[k]=label_list    \n",
    "print(len(dis_agree_list))\n",
    "print(len(uniq_id))\n",
    "Counter(agree_label_bucket)\n",
    "\n",
    "#28051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28050\n",
      "404\n",
      "404\n",
      "28050\n"
     ]
    }
   ],
   "source": [
    "print(28091-41)\n",
    "print(9+4+11+17+333+7+1+1+21)\n",
    "print(333+9+21+41)\n",
    "print(28495-363-(41+41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28091\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(dup_entries))\n",
    "print(len(dis_agree_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name=\"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data.tsv\"\n",
    "out_file=open(out_file_name,\"w\")\n",
    "out_file.write(\"id\\timage_path\\tdamage_type_human\\tdamage_type_classified\\tdamage_level_classified\\tdamage_level_human\" + \"\\n\")\n",
    "unique_id_list=[]\n",
    "\n",
    "for k,val in dup_entries.iteritems():\n",
    "    for inst in val:\n",
    "        img_id=inst.id\n",
    "        if(img_id in dis_agree_list):\n",
    "            continue\n",
    "        if(img_id not in unique_id_list):\n",
    "            if(inst.damage_type_classified==\"no\"):\n",
    "                damage_type_classified=\"no_damage\"\n",
    "            else:\n",
    "                damage_type_classified=inst.damage_type_classified\n",
    "\n",
    "            out_file.write(img_id+\"\\t\"+inst.image_path+\"\\t\"+inst.damage_type_human+\"\\t\"+damage_type_classified+\"\\t\"+inst.severity_label_classified+\"\\t\"+inst.severity_label_human+\"\\n\")        \n",
    "            unique_id_list.append(img_id)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mild and severe -- human annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_name=\"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data_human_mild_severe.csv\"\n",
    "out_file=open(out_file_name,\"w\")\n",
    "out_file.write(\"image,machine_label,human label\" + \"\\n\")\n",
    "unique_id_list=[]\n",
    "\n",
    "for k,val in dup_entries.iteritems():\n",
    "    for inst in val:\n",
    "        img_id=inst.id\n",
    "        if(img_id in dis_agree_list):\n",
    "            continue\n",
    "        if(img_id not in unique_id_list):\n",
    "            if(inst.damage_type_classified==\"no\"):\n",
    "                damage_type_classified=\"no_damage\"\n",
    "            else:\n",
    "                damage_type_classified=inst.damage_type_classified\n",
    "            \n",
    "            image_url=inst.image_path.replace(\"/export/aidr-dev-data-02/aidr_data/persister/190830201918_hurricane_dorian_2019/\",\"https://aidr-dev2.qcri.org/hurricane_dorian/\")\n",
    "            if(inst.severity_label_human==\"none\"):\n",
    "                continue\n",
    "            out_file.write(image_url+\",\"+inst.severity_label_classified+\",\"+inst.severity_label_human+\"\\n\")        \n",
    "            unique_id_list.append(img_id)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(dup_entries_ve))\n",
    "\n",
    "out_file_name_dup=\"/Users/firojalam/QCRI/hurricane_dorian_2019/data_annotation/hurricane_dorian_annotated_data_with_dup.tsv\"\n",
    "out_file_dup=open(out_file_name_dup,\"w\")\n",
    "#out_file_dup.write(\"id\\timage_path\\tdamage_level_human\" + \"\\n\")\n",
    "\n",
    "dis_agree_list={}\n",
    "for k,val in dup_entries_ve.iteritems():\n",
    "    label_list = []\n",
    "    for inst in val:\n",
    "        if(inst.severity_label_human not in label_list):\n",
    "            label_list.append(inst.severity_label_human)\n",
    "    if(len(label_list)>=2):\n",
    "        dis_agree_list[k]=label_list\n",
    "    else:\n",
    "        #print(len(label_list))\n",
    "        continue        \n",
    "\n",
    "print(len(dis_agree_list))\n",
    "for k, val in dis_agree_list.iteritems():\n",
    "        inst = dup_entries_ve[k][0]\n",
    "        lab=\"\"\n",
    "        for label in val:\n",
    "           lab=lab+\"\\t\"+label\n",
    "        out_file_dup.write(inst.id+\"\\t\"+inst.image_path+\"\\t\"+lab.strip()+\"\\n\")\n",
    "out_file_dup.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n",
      "1086\n",
      "Counter({u'no_damage': 585, u'damage': 501})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(len(dont_know_human))\n",
    "print(len(dont_know_machine))\n",
    "counter=collections.Counter(dont_know_machine)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28495\n",
      "28495\n",
      "28495\n",
      "28495\n"
     ]
    }
   ],
   "source": [
    "print(len(damage_type_human_list))\n",
    "print(len(damage_type_classified_list))\n",
    "\n",
    "print(len(damage_level_human_list))\n",
    "print(len(damage_level_classified_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29581"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28495+1086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-405"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28090-28495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifaction_report(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        line=line.strip()\n",
    "        if(line==\"\"):\n",
    "            continue\n",
    "        row = {}\n",
    "        line=line.replace(\"micro avg\",\"micro_avg\")\n",
    "        row_data = re.split('\\s+', line)\n",
    "        #print(row_data)\n",
    "        row['class'] = row_data[0]\n",
    "        #print(row_data[1])\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    (P,R,F1,sumClassCnt)=(0,0,0,0)\n",
    "    for row in report_data:\n",
    "        tmp=row['precision']\n",
    "        P=P+(tmp*row['support'])\n",
    "        tmp=row['recall']\n",
    "        R=R+(tmp*row['support'])\n",
    "        tmp=row['f1_score']\n",
    "        F1=F1+(tmp*row['support'])\n",
    "        sumClassCnt=sumClassCnt+row['support']\n",
    "    precision=P/sumClassCnt;\n",
    "    recall=R/sumClassCnt;    \n",
    "    f1_score=F1/sumClassCnt;        \n",
    "    #print(str(precision)+\"\\t\"+str(recall)+\"\\t\"+str(f1_score)+\"\\n\")\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = metrics.accuracy_score(damage_level_human_list,damage_level_classified_list)\n",
    "report = metrics.classification_report(damage_level_human_list,damage_level_classified_list)\n",
    "P,R,F1=classifaction_report(report)\n",
    "print(str(acc)+\"\\t\"+str(P)+\"\\t\"+str(R)+\"\\t\"+str(F1))\n",
    "print(report)\n",
    "conf_mat = pd.crosstab(damage_level_human_list,damage_level_classified_list, rownames=['gold'], colnames=['pred'], margins=True)\n",
    "conf_mat_str = conf_mat.to_string(header=True)\n",
    "conf_mat_str = re.sub('\\s+', '\\t', conf_mat_str).strip()\n",
    "conf_mat_arr = re.split('\\n', conf_mat_str)\n",
    "for line in conf_mat_arr:\n",
    "    formatted_line = re.sub('\\s+', '\\t', line).strip()\n",
    "    conf_mat_data = conf_mat_data+formatted_line+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[  899   379   113]\n",
      " [ 5239 19652   721]\n",
      " [  402   380   710]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conf_mat = confusion_matrix(damage_level_human_list,damage_level_classified_list)\n",
    "print 'Confusion Matrix :'\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642042463590103\t0.889177048605\t0.766964730655\t0.803459203369\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     damage       0.26      0.74      0.39      2883\n",
      "         no       0.96      0.77      0.85     25612\n",
      "\n",
      "avg / total       0.89      0.76      0.81     28495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = metrics.accuracy_score(damage_type_human_list,damage_type_classified_list)\n",
    "report = metrics.classification_report(damage_type_human_list,damage_type_classified_list)\n",
    "P,R,F1=classifaction_report(report)\n",
    "print(str(acc)+\"\\t\"+str(P)+\"\\t\"+str(R)+\"\\t\"+str(F1)+\"\\n\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2124,   759],\n",
       "       [ 5960, 19652]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(damage_type_human_list,damage_type_classified_list)\n",
    "print 'Confusion Matrix :'\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
